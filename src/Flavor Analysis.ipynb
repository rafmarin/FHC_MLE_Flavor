{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01814984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\rafmarin\\PycharmProjects\\FHC_MLE_Flavor\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Collecting numpy>=1.17.0\n",
      "  Downloading numpy-1.21.4-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "Collecting scipy>=0.18.1\n",
      "  Downloading scipy-1.7.3-cp39-cp39-win_amd64.whl (34.3 MB)\n",
      "Installing collected packages: numpy, smart-open, scipy, Cython, gensim\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2 numpy-1.21.4 scipy-1.7.3 smart-open-5.2.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6788/348363729.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'pip install --upgrade gensim'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mre\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, random, nltk, os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3475e",
   "metadata": {},
   "source": [
    "Each row is a unique product. There are two columns, a de-identified company ID number and a flavor field. The flavor field may have multiple values in it that are comma separated (at first glance) so you may have to normalize it. Needs some scrubbing. We want to try to do a basic descriptive analysis first. See what is there. No one has ever looked. Number of uniques? Any common ones (value_counts)? Then we can think about embedding them and doing some clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Solliu\\Documents\\Deloitte\\Projects\\FHC\\FlavorData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf39e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the unique values for characterizing flavor. It seems like there are over 10,000 unique values for flavors.\n",
    "print(df['Characterizing_Flavor'].unique())\n",
    "print(df.Characterizing_Flavor.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f899647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the top 10 most common flavors\n",
    "df['Characterizing_Flavor'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc664bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's work on cleaning up the data. We see here a bunch of null values..\n",
    "df[df['Characterizing_Flavor'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533188a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's now do some scrubbing\n",
    "#words = {'vream':'cream', 'mallow':'marshmallow', 'rocksatar':'rockstar', 'crakcer':'cracker','starwberry':'strawberry',\n",
    "#         'srawberry':'strawberry ','cockt':'cocktail','cocunut':'coconut','lycehee':'lychee','cocunut':'coconut',\n",
    "#         'hawaiin':'hawaiian', 'forsted':'frosted','red, whiteandbiue':'red white and blue',\n",
    "#        'banana mentho':'banana menthol', 'craker':'cracker',\n",
    "#        'lrishcrea':'irish cream','cream cheese icin':'cream cheese icing'}\n",
    "\n",
    "\n",
    "df['Characterizing_Flavor'].fillna(value='Not Found', inplace= True) #sub null values\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.strip() #remove trailing and leading spaces\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace(r\"(?<=\\w)-(?=\\w)\", \" \")  # Replace dash between words\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.lower() #normalize case\n",
    "#df['Characterizing_Flavor'] = df['Characterizing_Flavor'].replace(words, regex=True) #fixing some typos\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace(',','')\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace('|','')\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace('-','')\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace(':','')\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace(r'\\s*[/mgl\\d\\s.]+$', '', regex=True) #removes trailing mg,numbers, characters\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].str.replace(r\"\\s+\", \" \") #remove multiple spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix typos\n",
    "replacements = {'mentho':'menthol','cocunut':'coconut','lycehee':'lychee','starwberry':'strawberry', 'srawberry':'strawberry',\n",
    "                'strawberrry':'strawberry', 'stawberry':'strawberry','koconut':'coconut','rocksatar':'rockstar',\n",
    "                'lrishcrea':'irish cream', 'icin':'icing', 'cockt':'cocktail','cocktai':'cocktail','coolmint':'cool mint',\n",
    "                'vream':'cream', 'crea':'cream', 'mallow':'marshmallow','crakcer':'cracker', 'orangey':'orange',\n",
    "                'whiteandbiue':'white and blue', 'watermelond':'watermelon','smoothi':'smoothie','coffe':'coffee',\n",
    "                'cherryb1osso':'cherryblosso','caramel apples':'caramel apple','bllueberry':'blueberry',\n",
    "                'banapanutbread':'banananutbread', 'strawberry and menthol':'strawberry menthol','menthol cotton candy':'cotton candy menthol',\n",
    "                'grizzly apple coo':'grizzly apple','grape fruit':'grapefruit','cirque du salt berries 20mg/ml 2x':'cirque du salt berries',\n",
    "                'cirque du salt berries 50mg/ml 2x':'cirque du salt berries', 'watermelon melon menthol':'watermelon menthol',\n",
    "                'cinnamon roll (la) diy':'cinnamon roll (la)','cinnabunns':'cinnabun','alaska elixirs loaffetti formerly funfetti':'funfetti'\n",
    "               }\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, j, text) # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "df['Characterizing_Flavor'] = df['Characterizing_Flavor'].apply(lambda x: replace_all(x, replacements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It looks like it changed # uniques and top 10! Nice. Let's keep going.\n",
    "print(df.Characterizing_Flavor.nunique())\n",
    "df['Characterizing_Flavor'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990e6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking a look at the least common flavors to see if theres anything else we need to scrub\n",
    "df['Characterizing_Flavor'].value_counts(ascending=True).head(59)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Characterizing_Flavor'].astype(str).str.contains('veno', regex = True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b20fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb[df_emb['Characterizing_Flavor'].astype(str).str.contains('venom', regex = True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Company_ID.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac333e",
   "metadata": {},
   "source": [
    "Generate texts from tokens in a column using the tokenizer function, which tokenizes text as well as removes stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer, stopwords):\n",
    "    tokens = tokenizer(text) # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [t.replace(\"'s\",\"\") for t in tokens] # remove 's\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b90386",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate tokens from text in tokens column\n",
    "\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "text_columns = [\"Characterizing_Flavor\"]\n",
    "\n",
    "df_emb = df.copy()\n",
    "df_emb.insert(2,\"tokens\",0)\n",
    "\n",
    "#convert to string\n",
    "for col in text_columns:\n",
    "    df_emb[col] = df_emb[col].astype(str)\n",
    "\n",
    "# tokenize text\n",
    "df_emb[\"tokens\"] = df_emb[\"Characterizing_Flavor\"].map(lambda x: tokenize_text(x, word_tokenize, custom_stopwords))\n",
    "print(df_emb)\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "df_emb_uniq = df_emb.copy()\n",
    "_, idx = np.unique(df_emb_uniq[\"tokens\"], return_index=True)\n",
    "df_emb_uniq = df_emb_uniq.iloc[idx, :]\n",
    "\n",
    "# Remove empty values and keep relevant columns\n",
    "df_emb = df_emb.loc[df_emb.tokens.map(lambda x: len(x) > 0), [\"Characterizing_Flavor\",\"tokens\"]]\n",
    "df_emb_uniq = df_emb_uniq.loc[df_emb_uniq.tokens.map(lambda x: len(x) > 0), [\"Characterizing_Flavor\",\"tokens\"]]\n",
    "\n",
    "\n",
    "print(f\"Original dataframe: {df.shape}\")\n",
    "print(f\"Pre-processed dataframe, w duplicates: {df_emb.shape}\")\n",
    "print(f\"Pre-processed dataframe, without duplicates: {df_emb_uniq.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9098a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_emb_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5c500",
   "metadata": {},
   "source": [
    "Now we will train a Word2Vec model using the tokens we generated earlier. This will output an array of vectors or number values for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbf0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_docs = df_emb.tokens.tolist()\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "\n",
    "## what do these represent? length of the list of documents, size of the generated vectors\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e503b40",
   "metadata": {},
   "source": [
    "How many of the tokenized words are making it into the trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b262956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check # words in the resulting tokenized words.\n",
    "flat_list = [item for sublist in tokenized_docs for item in sublist] #turn array of arrays into 1 array\n",
    "print(len(flat_list))\n",
    "unique_list = []\n",
    "[unique_list.append(x) for x in flat_list if x not in unique_list] #remove duplicates\n",
    "print(\"num unique words: \", len(unique_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index[\"flavor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e820c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index[\"strawberry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of vocab for the trained model\n",
    "#why are we getting less words for trained model?\n",
    "trained = []\n",
    "for item in model.wv.index_to_key:\n",
    "    trained.append(item)\n",
    "\n",
    "print(\"num of words in trained model: \", len(trained))\n",
    "\n",
    "unique_trained = []\n",
    "\n",
    "[unique_trained.append(x) for x in trained if x not in unique_trained] #remove duplicates\n",
    "print(\"num unique words: \", len(unique_trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a090020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('banana', 'menthol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f387997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"fruit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d70017",
   "metadata": {},
   "source": [
    "Implement PCA to look at the variance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a46ae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6788/2346017544.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvectorized_docs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#9390 examples with 100 variables for each example\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.shape(vectorized_docs) #9390 examples with 100 variables for each example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11338247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X , num_components):\n",
    "    #center the data by subtracting the mean\n",
    "    X_meaned = X - np.mean(X , axis = 0)\n",
    "\n",
    "    # calculating the covariance matrix of the mean-centered data.\n",
    "    cov_mat = np.cov(X_meaned , rowvar = False)\n",
    "\n",
    "    #Calculating Eigenvalues and Eigenvectors of the covariance matrix. A higher Eigenvalue corresponds to a higher variability.\n",
    "    #each column in the Eigen vector-matrix corresponds to a principal component\n",
    "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "    #sort the eigenvalues in descending order. arranging them in descending order of their Eigenvalue will automatically\n",
    "    #arrange the principal component in descending order of their variability.Hence the first column in our rearranged\n",
    "    #Eigen vector-matrix will be a principal component that captures the highest variability\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:,sorted_index]#similarly sort the eigenvectors\n",
    "\n",
    "    #take top values depending on # components\n",
    "    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n",
    "\n",
    "    #reduces dimension from 9390, 100 to 9390, 2\n",
    "    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n",
    "\n",
    "    print(X_reduced)\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2677ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6788/2377109410.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmat_reduced\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPCA\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_emb\u001B[0m \u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m#Creating a Pandas DataFrame of reduced Dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprincipal_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmat_reduced\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'PC1'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'PC2'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_emb' is not defined"
     ]
    }
   ],
   "source": [
    "mat_reduced = PCA(df_emb , 2)\n",
    "\n",
    "#Creating a Pandas DataFrame of reduced Dataset\n",
    "principal_df = pd.DataFrame(mat_reduced , columns = ['PC1','PC2'])\n",
    "\n",
    "#Concat it with target variable to create a complete Dataset\n",
    "principal_df = pd.concat([principal_df , df_emb['Characterizing_Flavor']] , axis = 1)\n",
    "\n",
    "principal_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5833504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "sb.scatterplot(data = principal_df , x = 'PC1',y = 'PC2' , hue = 'Characterizing_Flavor' , s = 60 , palette= 'icefire')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c143a6e",
   "metadata": {},
   "source": [
    "Let me try applying PCA to the original dataframe? That would make sense for being able to see all the different characterizing flavors...Can I vectorize without losing all the data first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7816068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating optimal value of k using elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e861bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "plt.plot()\n",
    "plt.xlim([0, .4])\n",
    "plt.ylim([0, .4])\n",
    "plt.title('Dataset')\n",
    "plt.scatter(vectorized_docs[0],vectorized_docs[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b30eb",
   "metadata": {},
   "source": [
    "Calculating values of Distortion and Intertia to find the optimal K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd545d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building the clustering model and calculating the values of the Distortion and Inertia\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 40)\n",
    "X = np.array(list(zip(vectorized_docs[0],vectorized_docs[1]))).reshape(len(vectorized_docs[0]), 2) #what does this X mean?\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / X.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / X.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0594077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the different values of Distortion\n",
    "for key, val in mapping1.items():\n",
    "\tprint(f'{key} : {val}')\n",
    "\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mapping2.items():\n",
    "    print(f'{key} : {val}')\n",
    "\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method using Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c724da2",
   "metadata": {},
   "source": [
    "To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion and is not rapidly changing anymore. Based on these graphs, the **optimal K value looks to be 5 (inertia) or 9 (distortion)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c60a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738ee99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb946b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX,\n",
    "    k,\n",
    "    mb,\n",
    "    print_silhouette_values,\n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#higher sil. coef is denser clusters\n",
    "#k is # clusters. look into intracluster distance(avg distance of all the points from the center), graph & elbow point\n",
    "#mini k vs k\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=4,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": vectorized_docs,\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7dc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(4):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa57b5",
   "metadata": {},
   "source": [
    "When k=5, Clusters 1 & 4 have almost exact top 10 representative words. 4 clusters seems to be the best in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2dc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "2442b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to do\n",
    "#clean data more\n",
    "#embedding failures. how much of the vocab is being represented?\n",
    "--not all words are in word2vec. are there words that are not being represented in there?\n",
    "--compare our vocab vs word2vec vocab. how much % of vocab is being represented in that vocab?\n",
    "\n",
    "#plot the clusters, pca to check variance of clusters, want to keep 70-80% of data, t-sne\n",
    "--pca: dimensionality reduction (e.g. groups columns into principal components)\n",
    "--chunking flavors that are mixed together\n",
    "--check dimensionality reduction is adequate\n",
    "--use 2 or 3 of vectors representing each number (right now there are 100 per word)\n",
    "--pca also used to determine how many cluster for k means\n",
    "\n",
    "#plot the clusters, coloring in by company ID\n",
    "\n",
    "#less important but also to do\n",
    "#k means vs mini k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea02d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9f794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emb[df_emb['Characterizing_Flavor'].astype(str).str.contains('banana', regex = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62479d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Characterizing_Flavor'].astype(str).str.contains('icin', regex = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1421812a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6788/1627286274.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Characterizing_Flavor'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'zombie'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mregex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df['Characterizing_Flavor'].astype(str).str.contains('zombie', regex = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe21c1a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}